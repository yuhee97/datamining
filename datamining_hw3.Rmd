---
title: "datamining-2020f homework_3"
author: " 4 조"
date: '2020 10 14'
output: html_document
---

#### 조원 : 1614335 통계학과 임나희, 1610812 통계학과 박혜영, 1611888 통계학과 박유희 **(최종 검토자 : 임나희)**

## 1. library load

```{r library}
# library load
# Auto data 
library(ISLR)
library(pROC)
library(boot)
```


### ch5) 5. (풀이 : 박혜영 / 기여 :임나희, 박유희)

#### (a)

```{r}
fit=glm(default~income + balance, data=Default, family = binomial)
summary(fit)
```

우선 p-값을 살펴보면 income과 balance의 경우 모두 p-값이 0.05보다 작기 떄문에 유의적인 것을 알 수 있다.
income의 계수의 추정값은 2.081e-05, balance의 계수 추정값은 5.647e-03라는 것을 알 수 있다. income 계수 추정값이 2.081e-05라는 것은 balance값이 고정되었을 때 income 값이 한 단위 증가하면 채무불이행할 오즈가 이전 채무불이행할 오즈의 exp(2.080898e-05)만큼 증가한다는 것을 알 수 있다. balance 계수 추정값이 5.647103e-03라는 것은 income 값이 고정되었을 때 balance 값이 한 단위 증가하면 채무불이행 할 오즈가 이전 채무불이행할 오즈의 exp(5.647e-03) 만큼 증가한다는 것을 알 수 있다 .

#### (b)-i

```{r}
#splitting datasets
set.seed(1)
n=nrow(Default)
ind.shfl=sample(1:n, size=n)
num.tr= floor(n*0.5)
num.val=n-num.tr
df.tr=Default[ind.shfl[1:num.tr],]
df.val=Default[ind.shfl[(num.tr+1):n],]
```

```{r}
#define (X, y)
x.tr=cbind(df.tr$income, df.tr$balance)
y.tr=df.tr$default
x.val=cbind(df.val$income, df.val$balance)
y.val=df.val$default
```

#### (b)-ii
```{r}
# training a mutiple logistic regression model on the training set
obj.glm=glm(formula = y.tr ~ x.tr, family=binomial)
summary(obj.glm)
```

우선 p-값이 살펴보면 x.tr1(df.tr의 income)과 x.tr2(df.tr의 balance)의 경우 모두 p-값이 0.05보다 작기때문에 유의적인 것을 알 수 있다.
x.tr1(df.tr의 income)의 계수 추정값은3.262e-05, x.tr2(df.tr의 balance) 계수 추정값은 5.689e-03  라는 것을 알 수 있다. x.tr1(df.tr의 income)의 계수 추정값이 3.262e-05라는 것은 x.tr2(df.tr의 balance) 값이 고정되었을 때 x.tr1(df.tr의 income) 값이 한 단위 증가하면 y.tr(df.tr의 default) 값이 3.262e-05만큼 증가한다는 것을 알 수 있다. x.tr2(df.tr의 balance) 계수 추정값이 5.689e-03라는 것은 x.tr1(df.tr$income) 값이 고정되었을 때 x.tr2(df.tr의 balance) 값이 한 단위 증가하면 y.tr(df.tr의 default) 값이 5.689e-03 만큼 증가한다는 것을 알 수 있다.
Auto 데이터셋에서 name 변수를 제외하고 새로운 Auto_name_ex을 생성하여 변수들 간의 상관관계를 확인했습니다.

음의 상관관계와 양의 상관관계를 갖는 변수들이 있음을 확인할 수 있습니다.

#### (b)-iii

```{r}
# predict y on the validation set, using predict()
y.val.hat.glm = cbind(1, x.val) %*% coef(obj.glm)
head(y.val.hat.glm)

# computing the posterior probability of predicted default
y.val.hat.glm.prob=exp(y.val.hat.glm)/(1+exp(y.val.hat.glm))

#classify the individual the to the default category
y.val.hat.glm.pred = ifelse(y.val.hat.glm.prob > 0.5, 'Yes', 'No')
``` 

####  (b)-iv
```{r}
#compute the validation set error
val.error=mean(y.val!=as.factor(y.val.hat.glm.pred))
val.error
```

검증 데이터셋에서는 Y의 예측값(적합된 모형기반)과 실제값을 비교하는 성능지표인 validation error를 계산한다. 모형 비교 과정에서 이와같은 validation error가 테스트 오류의 추정량 역할을 수행한다.

*validation error이 0.0254인 것으로 보아 Y의 예측값이 실제값과 다르게 나올 비율이 매우 낮은 것을 알 수 있다. 따라서 이 glm모형이 적절하다고 생각한다.


```{r}
#정오행렬
table1=table(y.val, y.val.hat.glm.pred)
table1

#민감도
table1[2,2]/(table1[2,1]+table1[2,2])

#특이도
table1[1,1]/(table1[1,1]+table1[1,2])

#정밀도
table1[2,2]/(table1[1,2]+table1[2,2])

#재현율
table1[2,2]/(table1[2,1]+table1[2,2])

```

이진 분류 문제에서의 대표적 오류 측도들에는 정오행렬, 민감도, 특이도, 정밀도, 재현율 등이 있다. 이러한 오류 측도들에 대해서는 맥락에 따라서 중요시 여기는 현상이 무엇인지, 혹은 task를 수행하지 못했을 때의 금전상의 손해가 어디서 가장 막심하게 일어나는지 등에 따라 목표로 하는 성능지표가 달라질 수 있다.

*민감도(=재현율)은 0.3121019이다. 민감도는 정오행렬에서 TP/P로 구할 수 있으며 이 예제에선 validation data set의 실제 Y(default)값이 "Yes"일 때, 올바르게 감지하는 능력을 나타낸다.

*특이도은 0.9960768이다. 특이도는 정오행렬에서 TN/N로 구할 수 있으며 이 예제에선 validation data set의 실제 Y(default)값이 "No"일 때, 올바르게 감지하는 능력을 나타낸다.


*정밀도은 0.7205882이다. 정밀도는 정오행렬에서 TP/D로 구할 수 있으며 이 예제에선 예측한 Y(default)값이 "Yes"일 때, 실제 Y값이 "Yes"일 비율을 나타낸다.


#### (b)-v
```{r}
#ROC 곡선 그리기
#install.packages("pROC")
r<-roc(y.val, y.val.hat.glm.prob)
plot.roc(r)
auc(r)
```

AUROC는 ROC곡선 아래 면적의 넓이 이다. 좋은 모형일 수록 AUROC가 1에 가깝고, ROC곡선이 형태가 왼쪽 위로 붙게 된다. 

*AUROC가 0.9419이고 ROC곡선이 왼쪽 위에 붙은 모양인 것을 보아 현재 glm모형이 좋은 모형이라고 볼 수 있을 거라 생각한다.


####(c)-1)

```{r}
#splitting datasets
set.seed(100)
n=nrow(Default)
ind.shfl=sample(1:n, size=n)
num.tr= floor(n*0.5)
num.val=n-num.tr
df.tr=Default[ind.shfl[1:num.tr],]
df.val=Default[ind.shfl[(num.tr+1):n],]

#define (X, y)
x.tr=cbind(df.tr$income, df.tr$balance)
y.tr=df.tr$default
x.val=cbind(df.val$income, df.val$balance)
y.val=df.val$default

# training a mutiple logistic regression model on the training set
obj.glm=glm(formula = y.tr ~ x.tr, family=binomial)
summary(obj.glm)
```

우선 p-값이 살펴보면 x.tr1(df.tr의 income)과 x.tr2(df.tr의 balance)의 경우 모두 p-값이 0.05보다 작기때문에 유의적인 것을 알 수 있다.
x.tr1(df.tr의 income)의 계수 추정값은 2.068e-05, x.tr2(df.tr의 balance) 계수 추정값은 5.782e-03 라는 것을 알 수 있다. x.tr1(df.tr의 income)의 계수 추정값이 2.068e-05라는 것은 x.tr2(df.tr의 balance) 값이 고정되었을 때 x.tr1(df.tr의 income) 값이 한 단위 증가하면 채무불이행할 오즈가 exp(2.068e-05)만큼 증가한다는 것을 알 수 있다. x.tr2(df.tr의 balance) 계수 추정값이 5.782e-03 라는 것은 x.tr1(df.tr의 income) 값이 고정되었을 때 x.tr2(df.tr의 balance) 값이 한 단위 증가하면 채무불이행할 오즈가 exp(5.782e-03) 만큼 증가한다는 것을 알 수 있다.


```{r}
# predict y on the validation set, using predict()
y.val.hat.glm = cbind(1, x.val) %*% coef(obj.glm)
head(y.val.hat.glm)

# computing the posterior probability of predicted default
y.val.hat.glm.prob=exp(y.val.hat.glm)/(1+exp(y.val.hat.glm))

#classify the individual the to the default category
y.val.hat.glm.pred = ifelse(y.val.hat.glm.prob > 0.5, 'Yes', 'No')

#compute the validation set error
val.error=mean(y.val!=as.factor(y.val.hat.glm.pred))
val.error

#정오행렬
table2=table(y.val, y.val.hat.glm.pred)
table2

#민감도
table2[2,2]/(table2[2,1]+table2[2,2])

#특이도
table2[1,1]/(table2[1,1]+table2[1,2])

#정밀도
table2[2,2]/(table2[1,2]+table2[2,2])

#재현율
table2[2,2]/(table2[2,1]+table2[2,2])

#ROC 곡선 그리기
r<-roc(y.val, y.val.hat.glm.prob)
plot.roc(r)
auc(r)
```

####  (c)-2)

```{r}
#splitting datasets
set.seed(500)
n=nrow(Default)
ind.shfl=sample(1:n, size=n)
num.tr= floor(n*0.5)
num.val=n-num.tr
df.tr=Default[ind.shfl[1:num.tr],]
df.val=Default[ind.shfl[(num.tr+1):n],]

#define (X, y)
x.tr=cbind(df.tr$income, df.tr$balance)
y.tr=df.tr$default
x.val=cbind(df.val$income, df.val$balance)
y.val=df.val$default

# training a mutiple logistic regression model on the training set
obj.glm=glm(formula = y.tr ~ x.tr, family=binomial)
summary(obj.glm)
```

우선 p-값이 살펴보면 x.tr1(df.tr의 income)과 x.tr2(df.tr의 balance)의 경우 모두 p-값이 0.05보다 작기때문에 유의적인 것을 알 수 있다.
x.tr1(df.tr의 income)의 계수 추정값은1.977e-05, x.tr2(df.tr의 balance) 계수 추정값은 5.557e-03 라는 것을 알 수 있다. x.tr1(df.tr의 income)의 계수 추정값이 1.977e-05라는 것은 x.tr2(df.tr의 balance) 값이 고정되었을 때 x.tr1(df.tr의 income) 값이 한 단위 증가하면 채무불이행할 오즈가 exp(1.977e-05)만큼 증가한다는 것을 알 수 있다. x.tr2(df.tr의 balance) 계수 추정값이 5.557e-03 라는 것은 x.tr1(df.tr의 income) 값이 고정되었을 때 x.tr2(df.tr의 balance) 값이 한 단위 증가하면 채무불이행할 오즈가 exp(5.557e-03) 만큼 증가한다는 것을 알 수 있다.


```{r}
# predict y on the validation set, using predict()
y.val.hat.glm = cbind(1, x.val) %*% coef(obj.glm)
head(y.val.hat.glm)

# computing the posterior probability of predicted default
y.val.hat.glm.prob=exp(y.val.hat.glm)/(1+exp(y.val.hat.glm))

#classify the individual the to the default category
y.val.hat.glm.pred = ifelse(y.val.hat.glm.prob > 0.5, 'Yes', 'No')

#compute the validation set error
val.error=mean(y.val!=as.factor(y.val.hat.glm.pred))
val.error

#정오행렬
table3=table(y.val, y.val.hat.glm.pred)
table3

#민감도
table3[2,2]/(table3[2,1]+table3[2,2])

#특이도
table3[1,1]/(table3[1,1]+table3[1,2])

#정밀도
table3[2,2]/(table3[1,2]+table3[2,2])

#재현율
table3[2,2]/(table3[2,1]+table3[2,2])

#ROC 곡선 그리기
r<-roc(y.val, y.val.hat.glm.prob)
plot.roc(r)
auc(r)
```

####  (c)-3)

```{r}
#splitting datasets
set.seed(10000)
n=nrow(Default)
ind.shfl=sample(1:n, size=n)
num.tr= floor(n*0.5)
num.val=n-num.tr
df.tr=Default[ind.shfl[1:num.tr],]
df.val=Default[ind.shfl[(num.tr+1):n],]

#define (X, y)
x.tr=cbind(df.tr$income, df.tr$balance)
y.tr=df.tr$default
x.val=cbind(df.val$income, df.val$balance)
y.val=df.val$default

# training a mutiple logistic regression model on the training set
obj.glm=glm(formula = y.tr ~ x.tr, family=binomial)
summary(obj.glm)
```

우선 p-값이 살펴보면 x.tr1(df.tr의 income)과 x.tr2(df.tr의 balance)의 경우 모두 p-값이 0.05보다 작기때문에 유의적인 것을 알 수 있다.
x.tr1(df.tr의 income)의 계수 추정값이2.081e-05, x.tr2(df.tr의 balance) 계수 추정값이 5.658e-03 라는 것을 알 수 있다. x.tr1(df.tr의 income)의 계수 추정값이 2.081e-05라는 것은 x.tr2(df.tr의 balance) 값이 고정되었을 때 x.tr1(df.tr의 income) 값이 한 단위 증가하면 채무불이행할 오즈가 exp(2.081e-05)만큼 증가한다는 것을 알 수 있다. x.tr2(df.tr의 balance) 계수 추정값이 5.658e-03 라는 것은 x.tr1(df.tr의 income) 값이 고정되었을 때 x.tr2(df.tr의 balance) 값이 한 단위 증가하면채무불이행할 오즈가 exp(5.658e-03) 만큼 증가한다는 것을 알 수 있다.


```{r}
# predict y on the validation set, using predict()
y.val.hat.glm = cbind(1, x.val) %*% coef(obj.glm)
head(y.val.hat.glm)

# computing the posterior probability of predicted default
y.val.hat.glm.prob=exp(y.val.hat.glm)/(1+exp(y.val.hat.glm))

#classify the individual the to the default category
y.val.hat.glm.pred = ifelse(y.val.hat.glm.prob > 0.5, 'Yes', 'No')

#compute the validation set error
val.error=mean(y.val!=as.factor(y.val.hat.glm.pred))
val.error

#정오행렬
table4=table(y.val, y.val.hat.glm.pred)
table4

#민감도
table4[2,2]/(table4[2,1]+table4[2,2])

#특이도
table4[1,1]/(table4[1,1]+table4[1,2])

#정밀도
table4[2,2]/(table4[1,2]+table4[2,2])

#재현율
table4[2,2]/(table4[2,1]+table4[2,2])

#ROC 곡선 그리기
r<-roc(y.val, y.val.hat.glm.prob)
plot.roc(r)
auc(r)
```

(1) 검증 데이터셋에서는 Y의 예측값(적합된 모형기반)과 실제값을 비교하는 성능지표인 validation error를 계산한다. 모형 비교 과정에서 이와같은 validation error가 테스트 오류의 추정량 역할을 수행한다.

*validation set error는 set.seed(100)일 때, 0.0264 / set.seed(500)일 때, 0.0308 /  set.seed(10000)일 때, 0.028이다. 따라서 set.seed(100)인 경우에 랜덤으로 생성한 인덱스로 나눈 validation data set을 가지고 glm모델을 적용하였을때, validation set error가 가장 작음을 알 수 있다. validation set error 관점에선 set.seed(100)일 때의 모형이 보다 좋은 모형이라고 할 수 있다.

(2) 이진 분류 문제에서의 대표적 오류 측도들에는 정오행렬, 민감도, 특이도, 정밀도, 재현율 등이 있다. 이러한 오류 측도들에 대해서는 맥락에 따라서 중요시 여기는 현상이 무엇인지, 혹은 task를 수행하지 못했을 때의 금전상의 손해가 어디서 가장 막심하게 일어나는지 등에 따라 목표로 하는 성능지표가 달라질 수 있다.

*정오행렬만을 보고 비교했을때는 set.seed(100), set.seed(500), set.seed(10000) 세 가지 경우에 크게 눈에 띄는 차이가 없어보인다. 

*민감도(재현율과 동일하다.)는 set.seed(100)일 때, 0.2911392 / set.seed(500)일 때, 0.2540541 /  set.seed(10000)일 때, 0.2923977이므로 큰 차이는 없으나 set.seed(10000)일 때 가장 크다. 따라서 set.seed(10000)인 경우에 랜덤으로 생성한 인덱스로 나눈 validation data set을 가지고 glm모델을 적용하였을때, 민감도가 가장 큰 것을 알 수 있다.  민감도 관점에선 set.seed(10000)일 때 보다 좋은 모형이라고 볼 수 있다.

*특이도는 set.seed(100)일 때, 0.9958695 / set.seed(500)일 때, 0.9966771 /  set.seed(10000)일 때, 0.9960654이므로 큰 차이는 없으나 set.seed(500)일 떄 가장 크다. 따라서 set.seed(500)인 경우에 랜덤으로 생성한 인덱스로 나눈 validation data set을 가지고 glm모델을 적용하였을때, 특이도가 가장 큰 것을 알 수 있다. 특이도 관점에선 set.seed(500)일 때 보다 좋은 모형이라고 볼 수 있다.

*정밀도는 set.seed(100)일 때, 0.6969697 / set.seed(500)일 때, 0.7460317 /  set.seed(10000)일 때, 0.7246377이므로 큰 차이는 없으나 set.seed(500)일 때 가장 크다. 따라서 set.seed(500)인 경우에 랜덤으로 생성한 인덱스로 나눈 validation data set을 가지고 glm모델을 적용하였을때, 정밀도가 가장 큰 것을 알 수 있다. 정밀도 관점에선 set.seed(500)일 때 보다 좋은 모형이라고 볼 수 있다.

(3) AUROC는 ROC곡선 아래 면적의 넓이 이다. 좋은 모형일 수록 AUROC가 1에 가깝고, ROC곡선이 형태가 왼쪽 위로 붙게 된다. 

*AUROC은 set.seed(100)일 때, 0.9463 / set.seed(500)일 때, 0.9534 /  set.seed(10000)일 때, 0.9473이므로 큰 차이는 없으나 set.seed(500)일 때 가장 1과 가깝다. 따라서 AUROC의 관점에선 set.seed(500)일 때 보다 좋은 모형이라고 볼 수 있다. (물론 세 모형 모두 AUROC가 1에 가깝고, ROC곡선의 형태가 왼쪽 위로 붙어있긴 하다.)


####  (d)
```{r}
#독립변수 student 추가 후 validation error
#splitting datasets
set.seed(1)
n=nrow(Default)
ind.shfl=sample(1:n, size=n)
num.tr= floor(n*0.5)
num.val=n-num.tr
df.tr=Default[ind.shfl[1:num.tr],]
df.val=Default[ind.shfl[(num.tr+1):n],]

#define (X, y)
x.tr=cbind(df.tr$income, df.tr$balance, df.tr$student)
y.tr=df.tr$default
x.val=cbind(df.val$income, df.val$balance, df.val$student)
y.val=df.val$default

# training a mutiple logistic regression model on the training set
obj.glm=glm(formula = y.tr ~ x.tr, family=binomial)
summary(obj.glm)
```


우선 x.tr2(df.tr의 balance)에 대한 p-값을 보면 0.05보다 작기때문에 x.tr2(df.tr의 balance)가 유의적이다. 하지만 x.tr1(df.tr의 income)와 x.tr3(df.tr의 student)의 p-값은 0.05보다 크기때문에 두 변수의 추가적인 설명력은 유의하지 않다고 볼 수 있다. 
x.tr1(df.tr의 income)의 계수 추정값이 1.686e-05, x.tr2(df.tr의 balance) 계수 추정값이 5.767e-03, x.tr3(df.tr의 student)의 계수 추정값이 -5.992e-01 라는 것을 알 수 있다. x.tr1(df.tr의 income)의 계수 추정값이 1.686e-05라는 것은 x.tr2(df.tr의 balance)과 x.tr3(df.tr의 student) 값이 고정되었을 때 x.tr1(df.tr의 income) 값이 한 단위 증가하면 채무불이행할 오즈가 exp(1.686e-05)만큼 증가한다는 것을 알 수 있다. x.tr2(df.tr의 balance) 계수 추정값이 5.767e-03 라는 것은 x.tr1(df.tr의 income)과 x.tr3(df.tr의 student) 값이 고정되었을 때 x.tr2(df.tr의 balance) 값이 한 단위 증가하면 채무불이행할 오즈가 exp(5.767e-03)  만큼 증가한다는 것을 알 수 있다. x.tr3(df.tr의 student)의 계수 추정값이 -5.992e-01라는 것은 x.tr1(df.tr의 income)과 x.tr2(df.tr의 balance) 값이 고정되었을 때 학생의 채무불이행할 오즈가 비학생의 채무불이행할 오즈의 exp(-5.992e-01)배가 된다는 것을 알 수 있다.


```{r}
# predict y on the validation set, using predict()
#y.val.hat.glm = predict(object = obj.glm, newdata = data.frame(x.val))
y.val.hat.glm = cbind(1, x.val) %*% coef(obj.glm)
head(y.val.hat.glm)

# computing the posterior probability of predicted default
y.val.hat.glm.prob=exp(y.val.hat.glm)/(1+exp(y.val.hat.glm))

#classify the individual the to the default category
y.val.hat.glm.pred = ifelse(y.val.hat.glm.prob > 0.5, 'Yes', 'No')

#compute the validation set error
val.error=mean(y.val!=as.factor(y.val.hat.glm.pred))
val.error

#정오행렬
table5=table(y.val, y.val.hat.glm.pred)
table5

#민감도
table5[2,2]/(table5[2,1]+table5[2,2])

#특이도
table5[1,1]/(table5[1,1]+table5[1,2])

#정밀도
table5[2,2]/(table5[1,2]+table5[2,2])

#재현율
table5[2,2]/(table5[2,1]+table5[2,2])

#ROC 곡선 그리기
r<-roc(y.val, y.val.hat.glm.prob)
plot.roc(r)
auc(r)
```

training data set을 이용하여 모형에 적합시킬 때와 validation data set을 이용하여 모형을 검증할 때 모두 student 변수를 추가하였다. 5.5(b)-iv와 비교해보면 set.seed(1)이 동일할 때 student 변수를 넣기 전과 후의 validation error이 각각 0.0254와 0.026이므로 큰 차이가 없다는 것을 알 수 있다.
따라서 student변수는 test error rate 감소에 영향을 주지 않을 것이라고 볼 수 있다.
또한 그 뿐만 아니라 student 변수를 넣기 전과 후의 정오행렬, 민감도, 특이도, 정밀도, 재현율, AUROC값에도 큰 차이가 없다는 것을 알 수 있다.



### ch5) 6. (풀이 : 임나희 / 기여 :박혜영, 박유희)
####  (a)
```{r}

data(Default)
set.seed(1)
lr<-glm(default~income+balance,family = binomial,data=Default)
summary(lr)$coefficient[ ,2]

```
standard error는 베타 제로햇은 4.347564e-01, 베타원햇은 4.98 5167e-06, 베타투햇은 2.273731e-04이다. 이는 값이 매우 작으므로 계수추정값의 추정량의 표준오차가 크지 않다는 것을 알 수 있다. 또한 계수 추정량의  standard error가 작아서 베타제로/베타원/베타투의 95% 신뢰구간이 작을것으로 예상된다
  
  
  
####  (b)
```{r}
set.seed(1)
boot.fn <- function(x,index=1:nrow(x)){coef(glm(default~income+balance,family = binomial,data=x,subset=index))}
boot.fn(Default)
```
 income의 계수 추정값은 2.080898e-05, balance의 계수 추정값은 5.647103e-03라는 것을 알 수 있습니다. 이는 income 계수 추정값이 2.080898e-05라는 것은 balance값이 고정되었을 때 income 값이 한 단위 증가하면 default의 오즈 값이 e^(2.080898e-05)배가 된다는 것을 알 수 있다. balance 계수 추정값이 5.647103e-03라는 것은 income 값이 고정되었을 때 balance 값이 한 단위 증가하면 default의 오즈 값이 e^(5.647103e-03 )배가 된다는 것을 알 수 있다 
 
 
####  (c)
```{r}

boot(Default, boot.fn , R=1000)
```



####  (d)
glm에서 구한 standard error는 income의 계수 추정값 베타원햇은 4.985167e-06, balance의 계수 추정값 베타투햇는 2.273731e-04이고 boot를 이용한 standard error는 income의 계수 추정값 베타원햇은  4.866284e-06, balance의 계수 추정값 베타투햇는 2.298949e-04이다. 따라서 glm 으로 구한 것과 boot로 구한 것에는 별 차이가 없음을 알 수 있다. 
glm에서 베타원햇의 se가 4.985167e-06이라는 것은 계수의 추정량의 standard error가 4.985167e-06임을 의미하고,베타투햇의 se가 2.273731e-04라는 것은 계수의 추정량의 standard error가 2.273731e-04임을 의미한다. standard error를 이용하여 모수의 신뢰구간 구하는 것에 이용할 수 있다.
또한 boot를 이용한 standard error에서 income 계수 추정값의 standard error가  4.866284e-06라는 것은 original dataset에서 레코드들을 랜덤으로 복원 추출하여 얻은 1000개의 새 데이터셋을 이용하여 생성한 모델에서 추정된 알파 햇 값들은  알파의 표본평균을 중심으로 하여 4.866284e-06을 표준편차로 분포하여 있음을 의미한다.   balance 계수 추정값의 standard error가 2.298949e-04라는 것은 original dataset에서 레코드들을 랜덤으로 복원 추출하여 얻은 1000개의 새 데이터셋을 이용하여 생성한 모델에서 추정된  알파 햇 값들은  알파의 표본평균을 중심으로 하여  2.298949e-04을 표준편차로 분표하여 있음을 의미한다.





### ch5) 8. (풀이 : 박유희 / 기여 :임나희, 박헤영)

#### (a)

```{r}
set.seed(1)
x = rnorm(100)
y = x - 2*x^2 + rnorm(100)
```

$$
Y = X - 2X^2 + \epsilon
$$
n: 100, rnorm() 함수를 이용하여 정규분포를 따르는 난수 100개를 생성함.

p: 2개, X와 X^2



#### (b)

```{r}
plot(x, y, col=c("blue"), main='scatter plot', xlab = 'X', ylab = 'Y', pch=16)
```

산점도를 보면 y와 x의 관계는 비선형으로, 해당 데이터는 단순선형회귀모형을 적용하는 것은 옳지 않다는 것을 확인할 수 있음.

또한, 포물선 형태의 모형이 더 적절하다는 사실을 확인할 수 있음.

#### (c)

```{r}

set.seed(1)

# 데이터프레임 생성
df = data.frame(x, x^2, x^3, x^4, y)

K =5 
mse.stack = matrix(0, nrow=K, ncol=4)
n = nrow(df)
idx = sample(1:n, size = n)

for (k in 1:K){
  
  inx = idx[(floor(n/K)*(k-1)+1):(floor(n/K)*k)]
  df.tr = df[-inx,]
  df.val = df[inx,]

  # i 모델
  lm1 = lm(y ~ x, data = df.tr)
  y.lm1.val.hat = cbind(1, df.val$x) %*% coef(lm1)
  mse.val.lm1 = mean((df.val$y - y.lm1.val.hat)^2)
  
  # ii 모델
  lm2 = lm(y ~ x + x.2, data = df.tr)
  y.lm2.val.hat = cbind(1, df.val$x, df.val$x.2) %*% coef(lm2)
  mse.val.lm2 = mean((df.val$y - y.lm2.val.hat)^2)
  
  # iii 모델
  lm3 = lm(y ~ .-x.4, data = df.tr)
  y.lm3.val.hat = cbind(1, df.val$x, df.val$x.2, df.val$x.3) %*% coef(lm3)
  mse.val.lm3 = mean((df.val$y - y.lm3.val.hat)^2)
  
  # iv 모델
  lm4 = lm(y ~ ., data = df.tr)
  y.lm4.val.hat = cbind(1, df.val$x, df.val$x.2, df.val$x.3, df.val$x.4) %*% coef(lm4)
  mse.val.lm4 = mean((df.val$y -y.lm4.val.hat)^2)
  
  mse = c(mse.val.lm1, mse.val.lm2, mse.val.lm3, mse.val.lm4)
  mse.stack[k,] = mse
}

mse.cv = apply(mse.stack, 2, mean)
names(mse.cv) = c("lm1", "lm2", "lm3", "lm4")
print(mse.cv)
```

각 모델에 사용된 설명변수들과 종속변수가 변수가 모두 포함된 데이터프레임을 생성함.

i 모델은 df.tr 데이터의 y, x 사용함.

ii 모델 df.tr 데이터의 y, x, x.2(= x^2) 사용함.

iii 모델 df.tr 데이터의 y, x, x.2(= x^2), x.3(= x^3) 사용함.

iv 모델 df.tr 데이터의 y, x, x.2(= x^2), x.3(= x^3), x.4(= x^4) 사용함.

5-fold 교차검증 후 각 모델의 mse 값을 평균낸 결과, 

lm1의 mse = 8.1796572, lm2의 mse = 0.9418378, lm3의 mse = 0.9641633, lm4의 mse = 0.9963195이며,

ii(= lm2)모형의 mse가 0.9418378로 가장 작았음. 따라서, ii 모델이 x와 y의 관계를 설명 및 예측하기에 가장 적합한 모델임을 확인할 수 있었음.



#### (d)

```{r}

set.seed(2)

# 데이터프레임 생성
df = data.frame(x, x^2, x^3, x^4, y)

K =5 
mse.stack = matrix(0, nrow=K, ncol=4)
n = nrow(df)
idx = sample(1:n, size = n)

for (k in 1:K){
  
  inx = idx[(floor(n/K)*(k-1)+1):(floor(n/K)*k)]
  df.tr = df[-inx,]
  df.val = df[inx,]

  # i 모델
  lm1 = lm(y ~ x, data = df.tr)
  y.lm1.val.hat = cbind(1, df.val$x) %*% coef(lm1)
  mse.val.lm1 = mean((df.val$y - y.lm1.val.hat)^2)
  
  # ii 모델
  lm2 = lm(y ~ x + x.2, data = df.tr)
  y.lm2.val.hat = cbind(1, df.val$x, df.val$x.2) %*% coef(lm2)
  mse.val.lm2 = mean((df.val$y - y.lm2.val.hat)^2)
  
  # iii 모델
  lm3 = lm(y ~ .-x.4, data = df.tr)
  y.lm3.val.hat = cbind(1, df.val$x, df.val$x.2, df.val$x.3) %*% coef(lm3)
  mse.val.lm3 = mean((df.val$y - y.lm3.val.hat)^2)
  
  # iv 모델
  lm4 = lm(y ~ ., data = df.tr)
  y.lm4.val.hat = cbind(1, df.val$x, df.val$x.2, df.val$x.3, df.val$x.4) %*% coef(lm4)
  mse.val.lm4 = mean((df.val$y -y.lm4.val.hat)^2)
  
  mse = c(mse.val.lm1, mse.val.lm2, mse.val.lm3, mse.val.lm4)
  mse.stack[k,] = mse
}

mse.cv = apply(mse.stack, 2, mean)
names(mse.cv) = c("lm1", "lm2", "lm3", "lm4")
print(mse.cv)

```

(c)와의 결과와 다름.

set.seed()의 seed 값이 다를 경우, idx = sample(1:n, size = n) 코드에 의해 새롭게 인덱스(= 행의 위치)가 뒤섞이기 떄문에 (c)에서의 값과 차이가 발생함.

단, (c)에서의 mse와 변동이 생겼지만 mse가 가장 작은 모델은 ii(= lm2)로 동일함.


#### (e)

```{r}

#       lm1       lm2       lm3       lm4 
# 8.1796572 0.9418378 0.9641633 0.9963195
```

(c)에서 mse가 가장 작은 모델은 lm2임. 

선형회귀모델인 lm1은 (b)에서의 산점도 결과를 살펴봤을 때, 부적절함을 확인했고 포물선 모형의 다중회귀모형이 적절할 것이라 생각했음.

산점도 그림을 살펴봤을 때와 동일하게 결과를 확인할 수 있었음.


### 추가 문제. (풀이 : 임나희, 박유희, 박혜영)


```{r}
############################################################
############## 테스트용 시뮬레이션 데이터 생성 #############
############################################################
# define the inverse logit function
expit = function(t) return(exp(t) / (1 + exp(t)))
# sample size is n=10000, the number of variables is p=2
n = 10000
X1 = rnorm(n)
X2 = rnorm(n)
# the true coefficients. beta0 = 1, beta1 = -2, beta2 = 1
theta.true = c(1, -2, 1)
# beta0 + beta1 * X1 + beta2 * X2
x_theta = theta.true[1] + theta.true[2] * X1 + theta.true[3] * X2           
# Y|X1,X2 follows the Bernoulli distribution 
#    with the success probability as expit(beta0 + beta1 * X1 + beta2 * X2)
y = rbinom(n=n, size=1, prob=expit(x_theta)) 


# 보조적인 개체
# 상수항(1)을 포함한 자료행렬. 아래 반복문에 필요할 겁니다.
Xmat = cbind(1, X1, X2)

############################################################
# 반복적 국소 이차 근사로 로지스틱 회귀분석 계수 구하기 시작
############################################################
# 최대 반복수. 코딩오류로 인한 무한루프를 막기 위하여 실무적으로 설정해줌
MAXITER = 1000
# 기존값과 갱신값이 tol 미만이면 수렴을 선언할 계획임. 실무적으로 10^-4 ~ 10^-6을 애용합니다.
tol = 10^-8
# 반복 갱신될 theta의 초기값 설정 
theta.old = c(0, 0, 0)
# 반복 알고리즘 시작
for (t in 1:MAXITER) {
  # 모니터링을 위한 프린트문
  cat(sprintf("============ Iteration %d ==========", t))
  
  ###########################################################
  # 여기에 코드를 채우세요.
  #     - R의 행렬/벡터 계산 기능을 이용하여, theta.old와 Xmat과 y로부터 theta.new를 계산하십시오.
  #   - 강의노트 4장의 20쪽 참조
  #   - 계산과정의 중간값들을 추가 변수로 정의해도 괜찮습니다.
  
  # 상수항, X1, X2의 계수 값 정보를 담고 있는 theta.old를 이용하여 beta0 + beta1*X1 + beta*X2를 생성하여,
  # 10000개 행을 x_theta_old에 할당합니다.
  x_theta_old = theta.old[1] + theta.old[2] * X1 + theta.old[3] * X2 
  
  # -y + exp(x_theta_old) / (1 + exp(x_theta_old)) 값을 a에 할당하여 10000개의 값을 가진 벡터를 생성합니다.
  a = -y + expit(x_theta_old)
  # 위에서 생성한 a vector와 Xmat matrix를 행을 기준으로 곱합니다.
  q_1 = (a * Xmat)
  # 3열 10000행을 가지고 있는 q_1을 열 기준으로 평균을 잰 후, q_1에 재할당합니다.
  q_1 = apply(q_1, 2, mean)
  
  # expit 함수에는 적용이 어려워 exp(x_theta_old) / ((1 + exp(x_theta_old))^2)를 할당하는 b vector를 새로 생성합니다.
  b = exp(x_theta_old) / ((1 + exp(x_theta_old))^2)
  # b vector와 Xmat matrix를 행을 기준으로 먼저 곱한 후, 앞의 행렬에 t(Xmat)(= Xmat 행렬의 전치행렬)을 곱하여 10000*10000 행렬을 생성합니다.
  q_2 = ((b * Xmat) %*% t(Xmat))
  # 행렬의 전체 값을 합하여 평균을 구한 후, q_2에 재할당합니다.
  q_2 = mean(q_2)
  # q_2의 역수를 구하여 q_2에 재할당합니다.
  q_2 = q_2^(-1)
  
  # 현재 theta.old에 q_2 vector와 q_1 matrix을 곱하여 만든 beta0, beta1, beta2 계수를 빼준 후 theta.new에 할당합니다.
  theta.new = theta.old - q_2 %*% q_1
  
  ###########################################################
  
  # diff는 theta.new와 theta.old간 유클리드 거리로 정의하였음
  diff = sqrt(sum((theta.new - theta.old)^2))
  sprintf("L2 difference between theta.new and theta.old: %.8f\n", diff)    

  # theta가 충분히 수렴한 듯하면 
  # 가장 최신의 theta.new를 theta의 추정값(theta.hat)으로 제시 후
  # 반복문 빠져나가기
  if (diff < tol) { 
    cat(sprintf("Fisher scoring algorithm converged with %d iterations\n", t))
    theta.hat = theta.new
    break
  }
  
  theta.old = theta.new
  
  # 수렴 실패하였을 경우 메시지 출력
  if (t == MAXITER) cat("Did not converge\n")
}

############################################################
######################### 테스트 ###########################
############################################################

# 첫번째 체크. n을 크게 설정할수록 theta.true에 가까운 값이 출력되어야 함
print(theta.hat)
# 두번째 체크. theta.hat과 동일한 값이 나와야 함
glm(y~ X1 + X2, family=binomial)                                        

```

코드 설명은 주석으로 달아놨습니다.

print(theta.hat)

glm(y~ X1 + X2, family=binomial)

실행 결과가 동일한 값이 나왔습니다.



